path:
  ckpt_path: "./output/ckpt/LJSpeech"
  log_path: "./output/log/LJSpeech"
  result_path: "./output/result/LJSpeech"

optimizer:
  batch_size_shallow: 16
  gamma: 0.999
  grad_clip_thresh: 1
  grad_acc_step: 1
  init_lr_G: 0.0001
  init_lr_D: 0.0002
  batch_size: 32
  betas: [0.9, 0.98]
  eps: 0.000000001
  weight_decay: 0.0
  warm_up_step: 2000
  anneal_steps: [50000, 100000, 150000]
  anneal_rate: 0.5
  init_lr: 0.001
optimizer_fs2:
  betas: [0.9, 0.98]
  eps: 0.000000001
  weight_decay: 0.0
  warm_up_step: 2000
  anneal_steps: [60000, 120000, 180000]
  anneal_rate: 0.3
loss:
  adv_loss_mode: "lsgan"
  noise_loss: "l1"
  dur_loss: "mse"
  pitch_loss: "l1"
  cwt_loss: "l1"
  # cwt_add_f0_loss: false
  lambda_f0: 1.0
  lambda_uv: 1.0
  lambda_ph_dur: 1.0
  lambda_word_dur: 0.0 # lambda_word_dur should not be activated, otherwise it will produce NaN value (For VCTK)
  lambda_sent_dur: 1.0
  lambda_d: 0.1
  lambda_p: 0.1
  lambda_e: 0.1
  lambda_fm: 10.0
  lambda_fm_shallow: 0.001
step:
  total_step_aux: 200000
  total_step_naive: 300000
  total_step_shallow: 400000
  total_step_diff_naive: 300000
  total_step_consistency_training: 1200000
  log_step: 100  # 100
  synth_step: 1000  # 1000
  val_step: 1000  # 1000
  save_step: 10000  # 20000
cm:
  training_mode: consistency_training
  target_ema_mode: fixed
  start_ema: 0.95
  scale_mode: fixed
  distill_steps_per_iter: 10
  start_scales: 3
  end_scales: 200
  total_training_steps: 800000
  loss_norm: l1 #  lpips、mel_loss、mel_lossl2、l1、l2、l2-32,+mel_loss
  lr_anneal_steps: 0
  teacher_model_path:
  attention_resolutions: 32,16,8
  class_cond: true
  use_scale_shift_norm: true
  dropout: 0.0
  teacher_dropout: 0.1
  ema_rate: 0.999,0.9999,0.9999432189950708
  global_batch_size: 2048
  image_size: 64
  lr: 0.0001
  num_channels: 192
  num_head_channels: 64
  num_res_blocks: 3
  resblock_updown: true
  use_fp16: false
  weight_decay: 0.0
  weight_schedule: uniform

  sigma_min: 0.002
  sigma_max: 80.0
  num_heads: 4
  num_heads_upsample: -1
  channel_mult: ""
  use_checkpoint: false
  use_new_attention_order: false
  learn_sigma: false
  microbatch: -1
  log_interval: 100
  save_interval: 100  #
  resume_checkpoint: null  #
  fp16_scale_growth: 1e-3
  schedule_sampler: linear12  # lognormal、uniform、loss-second-moment、linear12、linear21

  # synthesize_param
  sampler: onestep
  generator: determ
  num_samples: 10000
  seed: 42
  clip_denoised: false
  s_churn: 0.0
  s_tmin: 0.0
  s_noise: 1.0